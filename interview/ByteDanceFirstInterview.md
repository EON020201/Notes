# 字节跳动-大模型算法实习生-TikTok搜索-面试经历-一面

## 一、代码题目

**1.输入一个只有0和1的数组，最多可以将k个0转为1，求数组中最长连续1的个数。**

**2.二叉树层序遍历和Z字形遍历如何实现，口头描述。**

## 二、项目问题

**1.讲解一下Transformer的架构。**

https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html

**2.Transformer的解码器的第一个多头注意力为什么要加掩码？**

答：解码器用来预测文本，不能让解码器看到未来的信息。

**3.Transformer中还有哪些要加掩码的地方？**

答：在Transformer模型中，掩码（Masking）主要用于控制注意力机制的信息流动，以下是需要添加掩码的关键位置：

### 1. **编码器的填充掩码（Padding Mask）**
   - **作用**：屏蔽输入序列中的填充符（如`<pad>`），防止模型关注无效位置。
   - **场景**：当输入序列长度不一致时，通过填充符对齐长度。在自注意力计算时，需将填充符对应的注意力权重设为负无穷（或通过加法掩码），确保其不参与计算。
   - **实现方式**：
     ```python
     # 示例：生成填充掩码（1表示有效位置，0表示填充）
     padding_mask = (input_ids != pad_token_id).unsqueeze(1).unsqueeze(2)
     ```

### 2. **解码器的因果掩码（Causal Mask）**
   - **作用**：确保解码器的自注意力仅关注当前位置及之前的信息，避免“未来信息泄漏”。
   - **场景**：训练阶段生成目标序列时，解码器需按自回归方式逐词预测，每个位置只能依赖已生成的部分。
   - **实现方式**：
     ```python
     # 示例：生成下三角矩阵（1表示允许关注，0表示屏蔽）
     seq_length = target.size(1)
     causal_mask = torch.tril(torch.ones(seq_length, seq_length)).bool()
     ```

### 3. **解码器交叉注意力的填充掩码**
   - **作用**：屏蔽编码器输出中的填充符，确保解码器在交叉注意力中仅关注有效信息。
   - **场景**：解码器通过交叉注意力融合编码器输出时，需忽略编码器端的填充位置。
   - **实现方式**：复用编码器的填充掩码，直接传递给解码器的交叉注意力层。

### 4. **特定任务的动态掩码（如MLM）**
   - **作用**：在预训练任务（如BERT的掩码语言模型）中主动掩盖部分输入token，强制模型学习上下文推理。
   - **场景**：随机选择输入token替换为`[MASK]`，并在计算损失时仅考虑被掩盖的位置。
   - **实现方式**：
     ```python
     # 示例：随机掩盖15%的token
     masked_indices = torch.rand(input.shape) < 0.15
     input[masked_indices] = mask_token_id
     ```

### 掩码应用示意图
```
输入序列: [A, B, <pad>, <pad>]  
编码器掩码: [[1, 1, 0, 0]]  

目标序列: [X, Y, Z]  
解码器因果掩码:  
[[1, 0, 0],  
 [1, 1, 0],  
 [1, 1, 1]]  
```

### 总结
- **核心掩码**：编码器填充掩码、解码器因果掩码、交叉注意力填充掩码。
- **扩展场景**：任务特定掩码（如MLM）、多模态/多序列对齐掩码等。
- **实现关键**：通过加法操作（`attn_scores + mask`）或布尔掩码过滤无效注意力权重。

**4.PPO算法中为什么要用clip函数限制策略的更新？**

答：在PPO（Proximal Policy Optimization）算法中使用clip函数限制策略更新的主要原因如下：

### 1. **防止策略更新幅度过大**
   - **问题背景**：策略梯度方法中，若更新步长过大，新策略可能与旧策略差异显著，导致旧策略收集的数据失效，引发性能骤降（即“策略崩溃”）。
   - **解决方案**：通过clip函数将新旧策略的概率比率（\( r_t(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \)）限制在区间 \([1-\epsilon, 1+\epsilon]\)（如\(\epsilon=0.2\)）。这直接约束了策略更新的幅度，避免了传统方法（如TRPO）中复杂的二阶优化。

### 2. **维持重要性采样的有效性**
   - **重要性采样依赖**：PPO利用旧策略的数据更新当前策略，依赖新旧策略分布接近。若分布差异过大，重要性采样的方差会显著增加，影响学习效果。
   - **Clip的作用**：通过限制概率比率，确保新旧策略的分布差异可控，从而保持重要性采样的低方差特性。

### 3. **梯度稳定性**
   - **梯度问题**：当概率比率极大或极小时，梯度可能陡峭，导致梯度爆炸或消失。
   - **Clip的平滑效果**：限制比率范围后，梯度变化更加平滑，使训练过程更稳定。

### 4. **保守策略优化**
   - **目标函数设计**：PPO的优化目标为 \(\min\left(r_t(\theta) A_t, \ \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t\right)\)。当优势函数\(A_t\)为正时，比率超过\(1+\epsilon\)的部分被截断；当\(A_t\)为负时，比率低于\(1-\epsilon\)的部分被截断。这种设计确保更新始终保守，避免单步优化破坏策略性能。

### 5. **计算高效性**
   - **替代复杂约束**：TRPO通过KL散度约束更新步长，但需计算二阶导数，计算成本高。PPO的clip函数仅需一阶优化，简化了实现并提升了计算效率。

### 总结
PPO中的clip函数通过限制策略更新的幅度，解决了策略梯度方法中的稳定性问题，同时避免了复杂的优化过程。它在保证训练稳定性的基础上，兼顾了计算效率和重要性采样的有效性，成为PPO算法的核心创新之一。

**5.在应用强化学习的过程中，你觉得最困难的点在哪里？**

答：奖励函数的设计，如果奖励函数设计的不好，则会导致很难收敛。

**6.讲解一下LoRA微调的原理。**

答：https://zhuanlan.zhihu.com/p/650197598

**7.LoRA微调有哪些好处和优势？**

答：LoRA（Low-Rank Adaptation）是一种高效微调大型预训练模型的技术，其核心优势在于通过低秩矩阵分解减少训练参数，同时保持模型性能。以下是其主要好处和优势：

### 1. **参数高效，降低计算成本**
   - **低秩分解**：通过引入可训练的低秩矩阵（秩为\( r \)，远小于原始维度\( d \)），将参数更新量从\( d^2 \)降至\( 2dr \)。例如，若原始权重矩阵为1024×1024（约百万参数），使用\( r=8 \)时，LoRA仅需约16K参数，减少约98%的参数量。
   - **冻结主干参数**：仅训练新增的低秩矩阵，避免更新全部模型参数，显著降低计算和显存需求。

### 2. **节省存储与部署成本**
   - **轻量存储**：每个下游任务仅需保存LoRA适配器（通常几MB到几十MB），而非整个模型（可能数百GB）。例如，一个7B参数的模型微调后，LoRA模块可能仅需几十MB。
   - **多任务支持**：同一主干模型可搭配多个LoRA模块，切换任务时无需重新加载模型，仅需替换适配器，极大简化多任务部署。

### 3. **训练速度与效率提升**
   - **梯度计算优化**：仅需计算低秩矩阵的梯度，减少反向传播时间。实验表明，LoRA可将训练速度提升30%-50%（如GPT-3微调场景）。
   - **显存占用降低**：训练时仅需缓存小矩阵梯度，显存需求大幅减少，使得在消费级GPU（如RTX 3090）上微调大模型成为可能。

### 4. **缓解过拟合，增强泛化**
   - **隐式正则化**：低秩结构限制参数自由度，防止复杂任务中的过拟合。例如，在少样本场景（如100条训练数据）下，LoRA通常比全参数微调获得更高准确率。

### 5. **兼容性与灵活性**
   - **架构无关**：适用于Transformer、CNN等各类模型，可针对不同层（如自注意力、FFN）灵活注入适配器。
   - **与其他技术协同**：可与量化（QLoRA）、蒸馏等技术结合，进一步压缩模型或提升效率。

### 6. **保留预训练知识**
   - **主干参数冻结**：避免全参数微调导致的“灾难性遗忘”，确保模型在原始任务上的能力不受损，提升微调稳定性。

### 示例场景
- **大模型微调**：使用LoRA在单张24GB显卡上微调LLaMA-7B，仅需调整0.1%参数（约8M），训练速度提升2倍，显存占用减少60%。
- **多任务部署**：医疗和法律问答系统共享同一BERT主干，通过加载不同的LoRA模块实现任务切换，存储成本降低90%。

### 潜在局限与权衡
- **秩的选择敏感**：秩\( r \)需根据任务复杂度调整，过高可能导致冗余，过低可能限制模型能力。
- **理论性能上限**：极端复杂任务中，全参数微调可能优于LoRA，但多数场景下LoRA能以更低成本达到相近效果。

综上，LoRA通过参数高效、资源节约和部署灵活等优势，成为大模型微调的主流方案，尤其适合资源受限场景和多任务需求。

**8.LoRA微调为什么是高效的？**

答：LoRA（Low-Rank Adaptation，低秩适应）是一种高效微调大语言模型（如GPT-3、LLaMA等）的技术，其高效性主要体现在以下几个方面：

---

### 1. **参数效率：低秩分解减少可训练参数**
   - **传统微调的问题**：全量微调需要更新模型所有参数，对于大模型（如数十亿参数），训练和存储成本极高。
   - **LoRA的解决方案**：  
     LoRA冻结预训练模型的原始权重，仅在原始权重旁添加**低秩矩阵（Low-Rank Matrices）**来模拟参数更新。  
     例如，对权重矩阵 \( W \in \mathbb{R}^{d \times k} \)，LoRA将其更新分解为：  
     \[
     W' = W + \Delta W = W + BA \quad (B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, \text{且 } r \ll \min(d,k))
     \]
     - 秩（Rank）\( r \) 通常很小（如8或64），因此新增参数量从 \( d \times k \) 减少到 \( r \times (d + k) \)，显著降低训练负担。
   - **实际效果**：  
     例如，GPT-3 175B的全量微调需更新1750亿参数，而LoRA仅需更新约0.01%的参数（具体取决于秩 \( r \) 的选择）。

---

### 2. **计算效率：保持预训练权重固定**
   - **冻结原始参数**：LoRA不修改预训练模型的权重，仅训练新增的低秩矩阵，避免了反向传播中对大参数矩阵的梯度计算。
   - **减少显存占用**：由于大部分参数固定，显存消耗主要来自低秩矩阵和激活值（activations），适合在消费级GPU上微调大模型。

---

### 3. **避免灾难性遗忘**
   - 传统微调可能因过度调整参数导致模型遗忘预训练中学到的通用知识。
   - LoRA通过限制参数更新的自由度（低秩约束），使模型在适应新任务时更稳定，保留原有知识。

---

### 4. **灵活部署与模型切换**
   - **动态切换任务**：不同任务的LoRA适配器（低秩矩阵）可以独立训练，并在推理时动态加载，无需为每个任务保存完整模型副本。
   - **无缝合并权重**：训练完成后，低秩矩阵 \( BA \) 可直接与原始权重 \( W \) 合并，**不增加推理延迟**（与原始模型速度一致）。

---

### 5. **理论依据：参数更新的低秩特性**
   - 研究表明，大模型在下游任务中的参数更新矩阵通常是低秩的（即大部分信息可由少数主成分表示）。  
   - LoRA通过低秩分解捕捉核心任务相关变化，避免冗余参数更新。

---

### 对比其他高效微调方法
| 方法               | 参数量   | 推理延迟 | 训练速度 | 任务切换灵活性 |
|--------------------|---------|----------|----------|----------------|
| 全量微调           | 极高     | 无       | 慢       | 差             |
| Adapter Layers     | 中等     | 增加     | 中等     | 中等           |
| Prefix Tuning      | 低       | 无       | 中等     | 中等           |
| **LoRA**           | **低**  | **无**   | **快**   | **高**         |

---

### 总结
LoRA的高效性源于**低秩分解**的数学性质，通过极少的参数调整实现接近全量微调的效果，同时节省计算资源、降低显存需求，并支持灵活的任务切换。这使得它成为大模型轻量级微调的首选技术之一。

**9.你用LoRA微调Qwen2.5模型是要做什么？**

答：熟悉LoRA微调流程，走一遍流程。

**10.怎么想到要去从零实现Llama2训练？**

答：目前大部分都是调用接口API，想要锻炼动手能力，深究模型细节，了解输入数据走向和维度变化

**11.从零实现Llama2训练这个项目带给你的收获，有哪些有意思的地方？**

答：Llama2模型有很多创新点，比如将归一化层从layernorm改成rmsnorm，将传统的加性位置编码信息改为旋转矩阵位置编码信息等。

**12.上次实习是在大四，这是你研究生第一次实习？**

答：对。

**13.最近在看哪些东西，有什么有意思的吗？**

答：在看具身智能方面的内容，后续想要将大语言模型与具身智能结合。

## 三、问面试官的问题

**1.我有哪些可以改进的地方？**

答：代码能力，按照题型刷力扣题。

**2.你们是做什么的？**

答：大模型生成数据，提取回答中有用的信息，修改数据格式。