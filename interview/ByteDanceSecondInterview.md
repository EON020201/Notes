# 字节跳动-大模型算法实习生-TikTok搜索-面试经历-二面

## 一、代码题目

**1.给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。子数组是数组中的一个连续部分。**

答案：https://leetcode.cn/problems/maximum-subarray/description/

## 二、项目问题

**1.给我介绍一下你那个分层强化学习的实时博弈对抗系统。**

答：主要是先搭建环境，我们用 JSBSIM 这个开源的飞行动力学模型，基于Openai的 gym 框架，重写环境结构，把自定义的状态观测空间、动作奖励等放进去。环境定义好后就可以训练，训练采用分层的强化策略。底层策略让飞机能正常飞行，先不管对抗的事；上层策略处理作战信息，根据当前态势输出飞机需要实现的俯仰角和横滚角，再把这些角度输出给底层策略去执行。

**2.你这块讲到的分层策略，底层策略负责平稳飞行，向上层输出对应的俯仰角等。你训练这个是为了做什么呀？**

答：为了实现无人机对抗效果，提升无人机在作战环境中的自主决策能力。把无人机当作一个智能体，让它和没有经过训练、采用随机策略的无人机对抗，要是能取胜，就达到目的了。在自博弈训练前，双方都是随机状态，我们的飞机是红方，设定了胜利的终止条件。作战模式有两种，一种不带导弹，就是 “狗斗”，两架无人机不带武器互相缠斗，目标是让自己的航向正对着对方飞机尾部，这样在实战中就能发射导弹；另一种带导弹，直接发射导弹，打中对方就获胜。

**3.那你继续介绍这个项目吧**

答：后面就是自博弈。在训练过程中，一开始双方都没经过训练，我方用 MAPPO 算法训练，对方采用随机策略。训练一轮后，我方胜率明显高于对方。接着下一轮训练，让对方采用我们上一轮训练好的策略，我们再进行训练，如此不断循环。大概训练两三轮后，每一百个回合，我方胜率基本持平，说明训练收敛了，因为很难再打败采用相同策略的自己，差不多是五五开的局面。最后我们设置了 reward（奖励），分不同情况讨论，因为有带导弹和不带导弹的作战模式。带导弹时，有三个奖励：一个是姿态奖励，飞机的各个角度要在合理范围，根据它和对方飞机的攻击角（AO 角）、受击角（TA 角），通过计算公式判断它相对于敌方无人机是否处于优势，以此确定奖励；一个是高度奖励，不带导弹时，判定输的条件是飞机低于一定高度，我们设置了基于速度和高度的惩罚，在相对安全高度以下就给惩罚；还有 event 奖励，这个奖励比较稀疏，看最后的结果，如果我方坠毁就减 200，如果追击到对方就加 200。

**4.你这个坠毁跟刚才的高度惩罚不是同一个事情吗？**

答：刚才讲高度惩罚是低于一定高度就算坠毁，没解释清楚，坠毁是飞机的某些参数超出限制，比如加速度超重、失重等，不单纯是高度问题。姿态奖励是根据和对方无人机的相对角度计算的，通过计算公式判断，如果当前姿态优于对方，奖励大于 0；如果处于劣势，奖励小于 0，都是按规则计算的。

**5.如果要改进你这个项目中的奖励函数，你觉得有哪些改进方向？**

答：改善的方向可能是用神经网络拟合一个奖励模型。比如针对姿态奖励，设定一个网络，输入我方和对方的姿态，输出一个值，通过构建数据集进行监督训练得到模型。

**6.那你这个和你的规则相比好处在哪？**

答：好处是规则可能不完善，强化学习存在 reward hacking（奖励漏洞利用）问题，智能体可能利用奖励漏洞获取奖励，而这不是我们期望的。基于规则的话可能会出现这种情况，用神经网络拟合或许能解决这个问题。

**7.你能再解释一遍这个漏洞吗？我没太理解，就是奖励不太完善的地方。**

答：比如高度奖励，奖励判断条件是只要高于对方飞机就能一直获得奖励，要是对方飞机已经结束战斗（比如坠毁等情况）不动了，但我方飞机高度还是高于它，奖励就会一直累加，这就是利用漏洞获取奖励。

**8.那你的模型怎么能避免这种漏洞呢？**

答：我们设计了很多不同版本的奖励函数，做了很多测试，主要是调整奖励函数规则。用神经网络的话，还是取决于构建的数据集，如果数据集足够完善，应该可以避免这个问题。

**9.那你的数据集要怎么构造？**

答：拿姿态举例，构造我方无人机和对方无人机的角度样例，给出俯仰角、偏转角、滚转角等，对应的真值就是一个奖励分数。

**10.我们去人为定义这个分数，自己打分，是这样吗？那你的意思是参考之前的一些经验？**

答：对，也可以结合先验知识。比如我们知道飞机正对着敌方无人机尾部时的角度，这种情况下可以直接给较大奖励。构造数据集的规模大概至少得上万条数据，具体取决于模型的参数量。

**11.我想知道你为什么这么判断，以及你用什么模型，大概需要多少数据？**

答：这取决于网络的输入以及输入数据之间的关系，看它们有没有高维特征，判断这些关系来决定网络的深度，构建一个能恰到好处提取数据特征的网络。

**12.从实际例子来讲可能好理解些。比如你现在要训练一个 reward 模型，把你能想到的特征都写一下，还有你想用的模型是什么，为什么用这个模型，最后数据集大概要构造多大，按逻辑串一遍。**

答：拿姿态举例，首先获取我方无人机的横滚角、俯仰角、偏航角，以及对方无人机的这三个角度，再计算它们之间的 AO 角和 TA 角。针对姿态的特征应该就是这些，一共 8 个角度。考虑构建的网络模型，我觉得直接用 MLP（多层感知机）就行，因为这些特征不算太复杂。大概三层，输入维度是 8，中间一个隐藏层，神经元数量可以是输入维度的 4 倍或 8 倍，比如 32、64，输出维度是 1。分开计算不同奖励，在调试时就能知道根据高度、角度分别获得多少奖励，这样方便构造数据集。

**13.介绍一下PPO算法。**

答：PPO 算法是一种 actor - critic 方法。actor 用了重要性采样原理，每次更新策略的幅度不能太大。actor 的损失函数主要是优势函数乘上一个值，优势函数定义为动作价值减去状态价值，计算方式通常用广义优先估计（GAE）。先算出当前价值函数的值，再用这个值减去当前奖励加上折扣因子（伽马）乘以下一步动作采取的奖励。PPO 算法有两种变体，我们项目用的是 CLIP 变体，直接裁剪，让新策略和旧策略动作概率的比值限制在 1 - ε 到 1 + ε 之间。critic 就是让价值函数去逼近 target，target 是当前奖励加上衰减因子乘以下一步动作可能得到的最大奖励。

**14.接下来我们会问一些之前的面试题。我看到之前的面试有个问题回答得不太好，想再问一遍。BERT 的预训练有哪些 mask 方式？**

答：BERT 预训练的 mask 方式有完形填空形式，就是在文本中间挖掉一些词；还有判断两个句子是否是连续上下文的方式。

**15.那它做完形填空，进行源码预测的时候，mask 的方式有哪些？**

答：有一种是直接随机掩蔽掉 15% 的词；还有一种动态的方式，每次训练生成新的掩蔽，第一次训练掩蔽可能固定，第二次随机。还有可能根据词出现的频率来掩蔽，如果词出现频率高，就不掩蔽，放在上下文中；如果频率低就掩蔽掉，这样能强调模型学习低频词汇的能力，这是看 BERT 相关论文时提到的。

**16.encoder 和 decoder 有什么区别？**

答：decoder 有两个多头注意力层，且不太一样。第一个多头注意力层比较正常，第二个是带掩蔽的，这个掩蔽是为了防止它在解码时看到未来的信息作弊。第二层是交叉注意力层，query 来自解码器的输出，KV 来自编码器的输出。

**17.接下来再问一些机器学习和深度学习的问题。为什么要进行归一化？**

答：为了让模型更容易收敛，避免梯度爆炸的风险。

**18.你了解哪些归一化的方法？**

答：layernorm 和 batchnorm。

**19.常用的防止过拟合的方法有哪些？**

答：有dropout，还有加惩罚项，在损失函数里加参数的惩罚项。数据初始化也有点关系，权重矩阵初始化，如果符合某种分布，可能让模型离最优解更近。还有 k 折交叉验证，把数据分成 k 份，每次取一份作为验证集，其他作为训练集。还有数据增强，比如图像处理里可以对数据进行翻转、裁切等操作生成新数据。

**20.你对一些比较经典的词嵌入方法了解吗？比如 word2Vector 是怎么做词嵌入的？**

答：这个说实话不太了解，我只知道 Transformer 里用 torch 模块里的 embedding。

**21.那我最后问一个机器学习的问题，逻辑回归的损失函数，你能讲一下吗？**

答：逻辑回归的损失函数公式是负的括号里真值乘上 log 预测值，再加上 1 减去真值乘上 log 括号 1 减去预测值（交叉熵损失函数）。


