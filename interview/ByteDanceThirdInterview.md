# 字节跳动-大模型算法实习生-TikTok搜索-面试经历-三面

## 一、代码题目

**1.用pytorch手写一个简单的神经网络及其训练流程代码。**

**2.手写强化学习PPO算法的伪代码。**

## 二、项目问题

**1.我看你简历里提到参加了一个强化学习的比赛，可以简单介绍一下算法吗？**

答：算法方面问题不大，主要是比赛的奖励函数，我在简历里着重写了，我们在这上面花了不少心思。

**2.你们和得第一名的方案有什么区别？**

答：区别的话，比赛代码都是闭源的，我们不知道其他人的代码。我觉得应该是奖励函数设计的问题，比赛有个社区，我们每天都在里面讨论奖励函数的设计方法，可能他们的奖励函数设计得比我们好。初赛相对简单，我们直接进决赛了。

**3.那你说一下在大模型微调这一块，哪个项目投入时间或者做的事情相对更深入一些？**

答：那应该是 LLAMA2 预训练。微调的话，我用 PEFT 库直接调用 LoRA 方法，定义几个超参数就做了。训练完成后，我就输入一段 prompt，让它输出几段文本，就做了这些。

**4.那假设遇到一个问题，要通过预训练提升大模型的效果，你会怎么做？**

答：做好预训练，首先要找一个开源数据集。模型方面要考虑用多少层，定义好每一层的参数。根据参数量找匹配的数据量，这样就可以做预训练。

**5.那比如做了两次预训练，中间有些变量不同，你怎么判断哪个效果好？**

答：我会用 save 保存模型参数权重，保留两个权重文件，分别加载后做前向推理，看哪个输出效果更好。

**6.但是全量推理的话，你怎么评估效果，用什么指标衡量？**

答：我会找一些公开数据集，让模型跑分，看正确率，谁的正确率高谁的效果就好。

**7.那常用的跑分任务有哪些？**

答：我了解到 OpenAI 有个数学数据集，叫 OpenAI Math，专门用来给推理模型跑分。把训练好的模型在上面跑，看正确率，比较哪个模型的正确率更高。

**8.另外，你简历提到经常浏览国内外论坛了解新技术变化，是吧？国内主要逛哪些？**

答：国内就经常逛知乎，上面分享的大佬很多，我主要看大模型方向的内容，最近在关注 DeepSpeed，看它的强化学习是怎么实现的。国外的话，偶尔会去看 Stack Overflow，代码调试出错的时候会去上面找解决方法。

**9.对于一些新的模型或技术，你有什么了解和看法吗？**

答：我对强化学习比较了解，最近有个新的强化学习方法 GRPO，它是在 PPO 的基础上优化掉了 critic 网络。给大模型输入一段文本，让它输出多个文本，对这些输出文本做归一化，即每个输出文本减去均值再除以方差，得到有正有负的值。让每个输入文本和不同的输出文本对反馈到 policy（大模型）里，优化大模型，增加奖励为正的文本输出概率，降低奖励为负的文本输出概率。

**10.这样做在哪些方面效果不错？**

答：主要是推理方面。现在推理模型很火，通用模型在解决常识性问题上表现还行，但在数学、代码这些逻辑性强的问题上表现不太好。他们就用强化学习的方法，比如让大模型做数学题，用思维链引导，给每个思维链步骤打分，回答对了给奖励，答错了给惩罚，以此提升大模型的推理能力。

**11.deepseek还有其他优势吗？**

答：我了解到它的训练成本很低，这是它的一个优势。

**12.为什么deepseek成本低呢？**

答：这个我没太关注过。


